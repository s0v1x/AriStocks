{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAZgef5zIQQn"
      },
      "source": [
        "# **This notebook groups the full execution of RNNs models (LSTM, GRU, BiLSTM, and BiGRU) in one function 'model_DoItAll()'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HakE_Ywzzp28"
      },
      "source": [
        "# **Install and import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hab1tg3tIqz"
      },
      "outputs": [],
      "source": [
        "!pip3 install mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRvhKMhXzyKj"
      },
      "source": [
        "**Preparing Databricks environment** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STwrvtpSuE0R"
      },
      "outputs": [],
      "source": [
        "!databricks configure --host https://community.cloud.databricks.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQO94qgi0O3Z"
      },
      "source": [
        "**Initializing the experiment on Databricks using mlflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC9Drb6XuVa5"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "mlflow.set_experiment(\"/Users/username@entity.ex/Experiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1vSFllER5bA"
      },
      "source": [
        "# **Data Exploring**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57BQIfqcy_HY"
      },
      "source": [
        "**Importing the libraries we will be working with.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIJkL1ZqHfO-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential, layers, callbacks\n",
        "from tensorflow.keras.layers import Dense, Dropout, GRU, LSTM, Bidirectional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_error\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN2bEwTL56Nj"
      },
      "source": [
        "## **Load data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPlAAniU0gcj"
      },
      "source": [
        "**We will be working with Apple's dataset from the period 17 March 2015, to 30 March 2021**\n",
        "\n",
        "**Note:** The dataset has been shifted already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "JnTuMfs1HS1P",
        "outputId": "44b2a108-1ab2-44e6-da36-cf7736cdbad3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-39ea73a1-3c78-4e40-9629-44fe53f67acc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>average_true_range40</th>\n",
              "      <th>ema_indicator40</th>\n",
              "      <th>ema_indicator25</th>\n",
              "      <th>sma_indicator50</th>\n",
              "      <th>sma_indicator45</th>\n",
              "      <th>open</th>\n",
              "      <th>ema_indicator50</th>\n",
              "      <th>ema_indicator10</th>\n",
              "      <th>acc_dist_index</th>\n",
              "      <th>adj_close</th>\n",
              "      <th>sma_indicator10</th>\n",
              "      <th>average_true_range45</th>\n",
              "      <th>sma_indicator40</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>ema_indicator15</th>\n",
              "      <th>average_true_range25</th>\n",
              "      <th>average_true_range35</th>\n",
              "      <th>ema_indicator30</th>\n",
              "      <th>daily_return</th>\n",
              "      <th>ema_indicator35</th>\n",
              "      <th>sma_indicator15</th>\n",
              "      <th>ema_indicator20</th>\n",
              "      <th>sma_indicator20</th>\n",
              "      <th>sma_indicator30</th>\n",
              "      <th>average_true_range50</th>\n",
              "      <th>average_true_range30</th>\n",
              "      <th>ema_indicator45</th>\n",
              "      <th>close</th>\n",
              "      <th>sma_indicator25</th>\n",
              "      <th>sma_indicator5</th>\n",
              "      <th>ema_indicator5</th>\n",
              "      <th>cumulative_return</th>\n",
              "      <th>sma_indicator35</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-03-17</th>\n",
              "      <td>0.699112</td>\n",
              "      <td>30.724633</td>\n",
              "      <td>31.251490</td>\n",
              "      <td>30.00315</td>\n",
              "      <td>30.313722</td>\n",
              "      <td>31.475000</td>\n",
              "      <td>30.396444</td>\n",
              "      <td>31.432506</td>\n",
              "      <td>4.093498e+07</td>\n",
              "      <td>28.953447</td>\n",
              "      <td>31.386750</td>\n",
              "      <td>0.698566</td>\n",
              "      <td>30.714937</td>\n",
              "      <td>31.830000</td>\n",
              "      <td>31.412500</td>\n",
              "      <td>31.465652</td>\n",
              "      <td>0.691004</td>\n",
              "      <td>0.698556</td>\n",
              "      <td>31.081118</td>\n",
              "      <td>1.672672</td>\n",
              "      <td>30.901729</td>\n",
              "      <td>31.693166</td>\n",
              "      <td>31.391584</td>\n",
              "      <td>31.917875</td>\n",
              "      <td>31.487500</td>\n",
              "      <td>0.699026</td>\n",
              "      <td>0.696796</td>\n",
              "      <td>30.555396</td>\n",
              "      <td>32.117500</td>\n",
              "      <td>31.817000</td>\n",
              "      <td>31.113499</td>\n",
              "      <td>31.355336</td>\n",
              "      <td>16.198665</td>\n",
              "      <td>31.126000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-18</th>\n",
              "      <td>0.699072</td>\n",
              "      <td>30.792578</td>\n",
              "      <td>31.318106</td>\n",
              "      <td>30.11425</td>\n",
              "      <td>30.420500</td>\n",
              "      <td>31.750000</td>\n",
              "      <td>30.463936</td>\n",
              "      <td>31.557051</td>\n",
              "      <td>1.728797e+08</td>\n",
              "      <td>29.279362</td>\n",
              "      <td>31.385000</td>\n",
              "      <td>0.698542</td>\n",
              "      <td>30.838375</td>\n",
              "      <td>32.290001</td>\n",
              "      <td>31.592501</td>\n",
              "      <td>31.547133</td>\n",
              "      <td>0.691264</td>\n",
              "      <td>0.698526</td>\n",
              "      <td>31.147981</td>\n",
              "      <td>1.125630</td>\n",
              "      <td>30.969272</td>\n",
              "      <td>31.687833</td>\n",
              "      <td>31.460719</td>\n",
              "      <td>31.914750</td>\n",
              "      <td>31.569333</td>\n",
              "      <td>0.698996</td>\n",
              "      <td>0.696819</td>\n",
              "      <td>30.623313</td>\n",
              "      <td>31.875000</td>\n",
              "      <td>31.881500</td>\n",
              "      <td>31.424999</td>\n",
              "      <td>31.609391</td>\n",
              "      <td>17.506631</td>\n",
              "      <td>31.264071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-19</th>\n",
              "      <td>0.693158</td>\n",
              "      <td>30.845379</td>\n",
              "      <td>31.360944</td>\n",
              "      <td>30.22045</td>\n",
              "      <td>30.516500</td>\n",
              "      <td>32.187500</td>\n",
              "      <td>30.519272</td>\n",
              "      <td>31.614860</td>\n",
              "      <td>9.451222e+06</td>\n",
              "      <td>29.058285</td>\n",
              "      <td>31.412250</td>\n",
              "      <td>0.693297</td>\n",
              "      <td>30.950562</td>\n",
              "      <td>32.312500</td>\n",
              "      <td>31.850000</td>\n",
              "      <td>31.588116</td>\n",
              "      <td>0.682114</td>\n",
              "      <td>0.691782</td>\n",
              "      <td>31.194886</td>\n",
              "      <td>-0.755040</td>\n",
              "      <td>31.019590</td>\n",
              "      <td>31.639166</td>\n",
              "      <td>31.500175</td>\n",
              "      <td>31.902875</td>\n",
              "      <td>31.635500</td>\n",
              "      <td>0.694266</td>\n",
              "      <td>0.689009</td>\n",
              "      <td>30.677735</td>\n",
              "      <td>31.475000</td>\n",
              "      <td>31.907700</td>\n",
              "      <td>31.577500</td>\n",
              "      <td>31.697927</td>\n",
              "      <td>16.619409</td>\n",
              "      <td>31.351143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-20</th>\n",
              "      <td>0.696079</td>\n",
              "      <td>30.876092</td>\n",
              "      <td>31.369718</td>\n",
              "      <td>30.31120</td>\n",
              "      <td>30.605944</td>\n",
              "      <td>32.062500</td>\n",
              "      <td>30.556752</td>\n",
              "      <td>31.589431</td>\n",
              "      <td>-1.398124e+08</td>\n",
              "      <td>28.693634</td>\n",
              "      <td>31.394750</td>\n",
              "      <td>0.695890</td>\n",
              "      <td>31.034937</td>\n",
              "      <td>32.099998</td>\n",
              "      <td>31.290001</td>\n",
              "      <td>31.573977</td>\n",
              "      <td>0.687229</td>\n",
              "      <td>0.695160</td>\n",
              "      <td>31.212958</td>\n",
              "      <td>-1.254902</td>\n",
              "      <td>31.044891</td>\n",
              "      <td>31.596500</td>\n",
              "      <td>31.497777</td>\n",
              "      <td>31.857875</td>\n",
              "      <td>31.685166</td>\n",
              "      <td>0.696580</td>\n",
              "      <td>0.693042</td>\n",
              "      <td>30.712398</td>\n",
              "      <td>31.802500</td>\n",
              "      <td>31.902100</td>\n",
              "      <td>31.693000</td>\n",
              "      <td>31.623618</td>\n",
              "      <td>15.155950</td>\n",
              "      <td>31.401143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-23</th>\n",
              "      <td>0.690864</td>\n",
              "      <td>30.921283</td>\n",
              "      <td>31.403009</td>\n",
              "      <td>30.38780</td>\n",
              "      <td>30.719222</td>\n",
              "      <td>31.780001</td>\n",
              "      <td>30.605605</td>\n",
              "      <td>31.628170</td>\n",
              "      <td>-1.341414e+08</td>\n",
              "      <td>28.992193</td>\n",
              "      <td>31.396500</td>\n",
              "      <td>0.691259</td>\n",
              "      <td>31.123875</td>\n",
              "      <td>31.962500</td>\n",
              "      <td>31.629999</td>\n",
              "      <td>31.602542</td>\n",
              "      <td>0.679240</td>\n",
              "      <td>0.689227</td>\n",
              "      <td>31.250993</td>\n",
              "      <td>1.040508</td>\n",
              "      <td>31.086980</td>\n",
              "      <td>31.565166</td>\n",
              "      <td>31.526798</td>\n",
              "      <td>31.785500</td>\n",
              "      <td>31.754166</td>\n",
              "      <td>0.692399</td>\n",
              "      <td>0.686190</td>\n",
              "      <td>30.759794</td>\n",
              "      <td>31.672501</td>\n",
              "      <td>31.903400</td>\n",
              "      <td>31.806000</td>\n",
              "      <td>31.683245</td>\n",
              "      <td>16.354157</td>\n",
              "      <td>31.472928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-24</th>\n",
              "      <td>3.694247</td>\n",
              "      <td>125.380130</td>\n",
              "      <td>123.938653</td>\n",
              "      <td>128.91260</td>\n",
              "      <td>128.934666</td>\n",
              "      <td>122.820000</td>\n",
              "      <td>125.770880</td>\n",
              "      <td>122.013795</td>\n",
              "      <td>8.195381e+09</td>\n",
              "      <td>119.886360</td>\n",
              "      <td>122.384999</td>\n",
              "      <td>3.687888</td>\n",
              "      <td>127.700250</td>\n",
              "      <td>122.900002</td>\n",
              "      <td>120.070000</td>\n",
              "      <td>122.547871</td>\n",
              "      <td>3.690351</td>\n",
              "      <td>3.697411</td>\n",
              "      <td>124.541857</td>\n",
              "      <td>-1.999351</td>\n",
              "      <td>125.022664</td>\n",
              "      <td>121.521999</td>\n",
              "      <td>123.243753</td>\n",
              "      <td>122.002499</td>\n",
              "      <td>124.892000</td>\n",
              "      <td>3.678568</td>\n",
              "      <td>3.696772</td>\n",
              "      <td>125.624960</td>\n",
              "      <td>120.589996</td>\n",
              "      <td>123.073600</td>\n",
              "      <td>121.307999</td>\n",
              "      <td>121.617514</td>\n",
              "      <td>339.367039</td>\n",
              "      <td>126.507714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-25</th>\n",
              "      <td>3.668391</td>\n",
              "      <td>125.146465</td>\n",
              "      <td>123.681064</td>\n",
              "      <td>128.74840</td>\n",
              "      <td>128.680444</td>\n",
              "      <td>119.540001</td>\n",
              "      <td>125.567708</td>\n",
              "      <td>121.754923</td>\n",
              "      <td>8.214703e+09</td>\n",
              "      <td>120.385513</td>\n",
              "      <td>122.247999</td>\n",
              "      <td>3.665046</td>\n",
              "      <td>127.163500</td>\n",
              "      <td>121.660004</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>122.303136</td>\n",
              "      <td>3.649137</td>\n",
              "      <td>3.667771</td>\n",
              "      <td>124.286899</td>\n",
              "      <td>0.416354</td>\n",
              "      <td>124.776404</td>\n",
              "      <td>121.552666</td>\n",
              "      <td>122.991014</td>\n",
              "      <td>121.982499</td>\n",
              "      <td>124.398666</td>\n",
              "      <td>3.658196</td>\n",
              "      <td>3.662213</td>\n",
              "      <td>125.406049</td>\n",
              "      <td>121.209999</td>\n",
              "      <td>122.708799</td>\n",
              "      <td>121.319998</td>\n",
              "      <td>121.275008</td>\n",
              "      <td>341.196363</td>\n",
              "      <td>126.126285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-26</th>\n",
              "      <td>3.640681</td>\n",
              "      <td>124.954442</td>\n",
              "      <td>123.490982</td>\n",
              "      <td>128.55480</td>\n",
              "      <td>128.332444</td>\n",
              "      <td>120.349998</td>\n",
              "      <td>125.396817</td>\n",
              "      <td>121.655846</td>\n",
              "      <td>8.288931e+09</td>\n",
              "      <td>121.004463</td>\n",
              "      <td>122.265999</td>\n",
              "      <td>3.640490</td>\n",
              "      <td>126.766500</td>\n",
              "      <td>121.480003</td>\n",
              "      <td>118.919998</td>\n",
              "      <td>122.166494</td>\n",
              "      <td>3.605572</td>\n",
              "      <td>3.636121</td>\n",
              "      <td>124.088389</td>\n",
              "      <td>0.514141</td>\n",
              "      <td>124.578271</td>\n",
              "      <td>121.538666</td>\n",
              "      <td>122.821394</td>\n",
              "      <td>121.979999</td>\n",
              "      <td>123.934666</td>\n",
              "      <td>3.636233</td>\n",
              "      <td>3.625473</td>\n",
              "      <td>125.223612</td>\n",
              "      <td>121.389999</td>\n",
              "      <td>122.362399</td>\n",
              "      <td>121.563998</td>\n",
              "      <td>121.253338</td>\n",
              "      <td>343.464736</td>\n",
              "      <td>125.663999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-29</th>\n",
              "      <td>3.595914</td>\n",
              "      <td>124.780567</td>\n",
              "      <td>123.329368</td>\n",
              "      <td>128.40440</td>\n",
              "      <td>127.939555</td>\n",
              "      <td>121.650002</td>\n",
              "      <td>125.239687</td>\n",
              "      <td>121.607510</td>\n",
              "      <td>8.265777e+09</td>\n",
              "      <td>121.184158</td>\n",
              "      <td>122.005999</td>\n",
              "      <td>3.600701</td>\n",
              "      <td>126.502249</td>\n",
              "      <td>122.580002</td>\n",
              "      <td>120.730003</td>\n",
              "      <td>122.069432</td>\n",
              "      <td>3.535349</td>\n",
              "      <td>3.585089</td>\n",
              "      <td>123.914299</td>\n",
              "      <td>0.148503</td>\n",
              "      <td>124.401145</td>\n",
              "      <td>121.873999</td>\n",
              "      <td>122.685070</td>\n",
              "      <td>121.659999</td>\n",
              "      <td>123.468666</td>\n",
              "      <td>3.600508</td>\n",
              "      <td>3.566290</td>\n",
              "      <td>125.056933</td>\n",
              "      <td>119.900002</td>\n",
              "      <td>122.177999</td>\n",
              "      <td>121.163998</td>\n",
              "      <td>121.298892</td>\n",
              "      <td>344.123293</td>\n",
              "      <td>125.224857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-03-30</th>\n",
              "      <td>3.569266</td>\n",
              "      <td>124.542491</td>\n",
              "      <td>123.065570</td>\n",
              "      <td>128.25960</td>\n",
              "      <td>127.428000</td>\n",
              "      <td>120.110001</td>\n",
              "      <td>125.030288</td>\n",
              "      <td>121.297054</td>\n",
              "      <td>8.295818e+09</td>\n",
              "      <td>119.696686</td>\n",
              "      <td>121.438999</td>\n",
              "      <td>3.576908</td>\n",
              "      <td>126.146250</td>\n",
              "      <td>120.400002</td>\n",
              "      <td>118.860001</td>\n",
              "      <td>121.798253</td>\n",
              "      <td>3.495135</td>\n",
              "      <td>3.554943</td>\n",
              "      <td>123.655312</td>\n",
              "      <td>-1.227446</td>\n",
              "      <td>124.151081</td>\n",
              "      <td>121.794666</td>\n",
              "      <td>122.419826</td>\n",
              "      <td>121.398999</td>\n",
              "      <td>123.025666</td>\n",
              "      <td>3.579098</td>\n",
              "      <td>3.531747</td>\n",
              "      <td>124.832719</td>\n",
              "      <td>122.150002</td>\n",
              "      <td>121.939599</td>\n",
              "      <td>120.635998</td>\n",
              "      <td>120.832595</td>\n",
              "      <td>338.671918</td>\n",
              "      <td>124.738856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1521 rows Ã— 34 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39ea73a1-3c78-4e40-9629-44fe53f67acc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39ea73a1-3c78-4e40-9629-44fe53f67acc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39ea73a1-3c78-4e40-9629-44fe53f67acc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            average_true_range40  ...  sma_indicator35\n",
              "date                              ...                 \n",
              "2015-03-17              0.699112  ...        31.126000\n",
              "2015-03-18              0.699072  ...        31.264071\n",
              "2015-03-19              0.693158  ...        31.351143\n",
              "2015-03-20              0.696079  ...        31.401143\n",
              "2015-03-23              0.690864  ...        31.472928\n",
              "...                          ...  ...              ...\n",
              "2021-03-24              3.694247  ...       126.507714\n",
              "2021-03-25              3.668391  ...       126.126285\n",
              "2021-03-26              3.640681  ...       125.663999\n",
              "2021-03-29              3.595914  ...       125.224857\n",
              "2021-03-30              3.569266  ...       124.738856\n",
              "\n",
              "[1521 rows x 34 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df = pd.read_csv('new_apple_f.csv', header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ortR5gxqH--w",
        "outputId": "6bfff8e7-df0e-4f1f-de5e-f042a7b9d7ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1521, 34)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkvPASvQQMGn"
      },
      "source": [
        "# **Get and prepare the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1sxpF1T--3U"
      },
      "outputs": [],
      "source": [
        "# split a multivariate dataset into samples to get Xtrain and ytrain | Xtest and ytest\n",
        "def split_data(X_dataset, y_dataset, n_steps_in):\n",
        "    \"\"\"\n",
        "    The function does the following:\n",
        "    (example with n_steps_in = 3)\n",
        "    Data = [\n",
        "    [ 10  15  25]\n",
        "    [ 20  25  45]\n",
        "    [ 30  35  65]\n",
        "    [ 40  45  85]\n",
        "    [ 50  55 105]\n",
        "    [ 60  65 125]]\n",
        "    \n",
        "    Treatment: \n",
        "    X = [\n",
        "    [10, 15]\n",
        "    [20, 25]\n",
        "    [30, 35]]\n",
        "    \n",
        "    y = [65]\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(X_dataset)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_steps_in\n",
        "    \n",
        "        # check if we are beyond the dataset\n",
        "        if end_ix > len(X_dataset):\n",
        "            break\n",
        "\n",
        "        # gather input and output parts of the pattern\n",
        "        X.append(X_dataset[i:end_ix, :])\n",
        "        y.append(y_dataset[end_ix-1:])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URaHhn8-L36W"
      },
      "outputs": [],
      "source": [
        "def get_data(full_dataset, cols_list, input_scaler, output_scaler, n_steps, percentage):\n",
        "    \"\"\"\n",
        "    This function prepares the data by splitting it into X_train and X_test for the given list of features.\n",
        "    And it splits the target into y_train and y_test, all with the giving percentage of split.\n",
        "    Moreover, it scales the values and make them suited to the RNN models (LSTM, Bi-LSTM, GRU, and Bi-GRU).\n",
        "\n",
        "    Arguments:\n",
        "    - full_dataset: Full dataset with all features and target variable.\n",
        "    - cols_list: list of features.\n",
        "    - input_scaler, output_scaler: scaler for the features set and for the target, respectively.\n",
        "    - n_steps: window size for the prediction models.\n",
        "    - percentage: splitting percentage of the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - Train and test datasets with selected list of features.\n",
        "    - The target variable for training and testing.\n",
        "    - The scaler used to normalize the values of ytrain. This will be the tool to inverse \n",
        "    the transformation for the predictions given by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Target variable\n",
        "    y = full_dataset[['close']]\n",
        "\n",
        "    ## Chosing columns\n",
        "    X = full_dataset[cols_list]\n",
        "\n",
        "    ## Choose the spliting percentage\n",
        "    split = int(len(y) * percentage)\n",
        "\n",
        "    ## Training Data\n",
        "    X_train = X.iloc[:split].values\n",
        "    y_train = y.iloc[:split].values\n",
        "\n",
        "    ## Test Data\n",
        "    X_test = X.iloc[split:].values\n",
        "    y_test = y.iloc[split:].values\n",
        "\n",
        "    ## Scale X_train and X_test\n",
        "    input_scaler.fit(X_train)\n",
        "    X_train_scaled = input_scaler.transform(X_train)\n",
        "    X_test_scaled = input_scaler.transform(X_test)\n",
        "    \n",
        "    # Scale y_train and y_test\n",
        "    output_scaler.fit(y_train.reshape(-1, 1))\n",
        "    y_train_scaled = output_scaler.transform(y_train.reshape(-1, 1))\n",
        "    y_test_scaled = output_scaler.transform(y_test.reshape(-1, 1))\n",
        "    \n",
        "    ## Setting variables shape for RNN models\n",
        "    Xtrain, ytrain = split_data(X_train_scaled, y_train_scaled, n_steps)\n",
        "    Xtest, ytest = split_data(X_test_scaled, y_test_scaled, n_steps)\n",
        "\n",
        "    return Xtrain, ytrain, Xtest, ytest, output_scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDDq5cUYPFkb"
      },
      "source": [
        "# **Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY3axb7oH5I5"
      },
      "outputs": [],
      "source": [
        "def Training(model_name, Xtrain, ytrain, Xtest, ytest, n_steps, Epochs, Batch_size, use_dropout = False):\n",
        "    \"\"\"\n",
        "    This function creates the model's architecture, and trains the model.\n",
        "\n",
        "    Arguments:\n",
        "    - model_name: name from the list [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\"]\n",
        "    - Xtrain, ytrain: The training data and its corresponding target values, respectively.\n",
        "    - Xtest, ytest: The test data and its corresponding target values, respectively.\n",
        "    - n_steps: window size for the prediction models.\n",
        "    - Epochs: Number of epochs for the model.\n",
        "    - Batch_size: The choosen batch_size for the model.\n",
        "    - use_dropout: Boolean variable indicating the use of Dropout in the architecture.\n",
        "\n",
        "    Returns:\n",
        "    The trained model, and its history after training.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the model name is one of the available models\n",
        "    assert model_name in [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\"]\n",
        "\n",
        "    # Get the number of features\n",
        "    n_features = Xtrain.shape[2]\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    #### --- LSTM Model ---\n",
        "    if model_name == \"LSTM\":\n",
        "        # 1st Hidden layer\n",
        "        model.add(LSTM(50, return_sequences = True, input_shape = (n_steps, n_features)))\n",
        "            \n",
        "        if use_dropout == True:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "        # 2nd Hidden layer\n",
        "        model.add(LSTM(100, return_sequences=False))\n",
        "\n",
        "        if use_dropout == True:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "    #### --- GRU Model ---\n",
        "    if model_name == \"GRU\":\n",
        "        # 1st Hidden layer\n",
        "        model.add(GRU(50, return_sequences = True, input_shape = (n_steps, n_features)))\n",
        "            \n",
        "        if use_dropout == True:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "        # 2nd Hidden layer\n",
        "        model.add(GRU(100, return_sequences=False))\n",
        "\n",
        "        if use_dropout == True:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "    #### --- BiLSTM Model ---\n",
        "    if model_name == \"BiLSTM\":\n",
        "        # 1st Hidden layer\n",
        "        model.add(Bidirectional(\n",
        "                  LSTM(50, return_sequences = True, \n",
        "                  input_shape = (n_steps, n_features))))\n",
        "                    \n",
        "        if use_dropout == True:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "        # 2nd Hidden layer\n",
        "        model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
        "\n",
        "        if use_dropout == True:\n",
        "            model.add(Dropout(0.2))\n",
        "    \n",
        "    #### --- BiGRU Model ---\n",
        "    if model_name == \"BiGRU\":\n",
        "        # 1st Hidden layer\n",
        "        model.add(Bidirectional(\n",
        "                  GRU(50, return_sequences = True, \n",
        "                  input_shape = (n_steps, n_features))))\n",
        "                    \n",
        "        if use_dropout == True:\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "        # 2nd Hidden layer\n",
        "        model.add(Bidirectional(GRU(100, return_sequences=False)))\n",
        "\n",
        "        if use_dropout == True:\n",
        "            model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss = 'mse', optimizer = 'adam', metrics = ['mse', 'mae', 'mape'])\n",
        "\n",
        "    ### Fit the model\n",
        "    # Create early stop after 10 epochs if 'val_loss' (mse) is not changing\n",
        "    early_stop = callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit(Xtrain, ytrain, epochs = Epochs, batch_size = Batch_size, \n",
        "                        validation_data = (Xtest, ytest), verbose = 0, callbacks = [early_stop])\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AgZ7QA07Vjt"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "def prediction(model, Xtest, outscaler):\n",
        "    \"\"\"\n",
        "    This function makes predictions based on the model and the test set given.\n",
        "\n",
        "    Arguments:\n",
        "    - model: The trained model returned by the Training() function. \n",
        "    - Xtest: The test dataset returned by get_data() function.\n",
        "    - outscaler: the scaler of the target variable, returned by get_data() function.\n",
        "\n",
        "    Returns: The predictions of the model.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(Xtest)\n",
        "    predictions = outscaler.inverse_transform(predictions)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWpcDNMyRLhN"
      },
      "outputs": [],
      "source": [
        "def model_DoItAll(model_name, full_dataset, cols_list, n_steps, Epochs, Batch_size, use_dropout, percentage):\n",
        "    \"\"\"\n",
        "    As the name indicates, this function does all the work.\n",
        "\n",
        "    Arguments:\n",
        "    - model_name: name from the list [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\"]\n",
        "    - full_dataset: Full dataset with all features and target variable.\n",
        "    - cols_list: list of features.\n",
        "    - n_steps: window size for the prediction models.\n",
        "    - Epochs: Number of epochs for the model.\n",
        "    - Batch_size: The choosen batch_size for the model.\n",
        "    - use_dropout: Boolean variable indicating the use of Dropout in the architecture.\n",
        "    - percentage: splitting percentage of the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - Test set and the predictions of the target variable.\n",
        "    - The trained model and its history on the evaluation.\n",
        "    (The function returns the model and its history to be saved on mlflow, at the end)\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the model name is one of the available models\n",
        "    assert model_name in [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\"]\n",
        "\n",
        "    # Define the scaler\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    in_scaler = MinMaxScaler()\n",
        "    out_scaler = MinMaxScaler()\n",
        "\n",
        "    # Create and get the data scaled\n",
        "    Xtrain, ytrain, Xtest, ytest, output_scaler = get_data(full_dataset, cols_list, in_scaler, out_scaler, n_steps, percentage)\n",
        "\n",
        "    # Get the values as a 1 dimensional array\n",
        "    ytrain = np.array([train[0] for train in ytrain])\n",
        "    ytest = np.array([test[0] for test in ytest])\n",
        "\n",
        "    # Train the model\n",
        "    model, model_history = Training(model_name, Xtrain, ytrain, Xtest, ytest, n_steps, Epochs, Batch_size, use_dropout)\n",
        "\n",
        "    ## Making predictions\n",
        "    ypred = prediction(model, Xtest, output_scaler)\n",
        "    ytest = output_scaler.inverse_transform(ytest)\n",
        "\n",
        "    return ytest, ypred, model, model_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdgQViZ9S7hf"
      },
      "outputs": [],
      "source": [
        "def evaluate(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    This function evaluates the performance of the model.\n",
        "    \n",
        "    Arguments:\n",
        "    - Test data of the target variable.\n",
        "    - Predictions made by the model.\n",
        "\n",
        "    Returns: Model performance based on: \n",
        "    - RMSE: Root Mean Squared Error.\n",
        "    - MAPE: Mean Absolute Percentage Error.\n",
        "    - MAE: Mean Absolute Error.\n",
        "    \"\"\"\n",
        "\n",
        "    RMSE = mean_squared_error(y_test, y_pred, squared = False)\n",
        "    MAPE = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
        "    MAE = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    return RMSE, MAPE, MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut3p9KWA9Z2B"
      },
      "outputs": [],
      "source": [
        "def turn_to_dataframe(model_name, dataset, ytest, ypred, percentage, step):\n",
        "    \"\"\"\n",
        "    This function creates dataframes out of the test data of the target and its predictions, based on the splitting percentage.\n",
        "    (This function is used when creating plots)\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the model name is one of the available models\n",
        "    assert model_name in [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\"]\n",
        "\n",
        "    y = dataset[['close']]\n",
        "    test_index = y[step-1:].iloc[int(len(y)*percentage):].index\n",
        "\n",
        "    ytest_df = pd.DataFrame(data=ytest, columns=['test_close'], index=test_index)\n",
        "    ypred_df = pd.DataFrame(data=ypred, columns=[model_name + 'predictions'], index=test_index)\n",
        "\n",
        "    return ytest_df, ypred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WWD8hgHP7RT"
      },
      "outputs": [],
      "source": [
        "def make_plot(dataset, ytest, ypred, percentage, model_name, step):\n",
        "    \"\"\"\n",
        "    This function makes plots for comparing the actual price and the predictions the chosen model that has been made.\n",
        "\n",
        "    Arguments:\n",
        "    - Test data of the target variable.\n",
        "    - Predictions made by the model.\n",
        "    - The model's name, for the model used in making prediction. Only \"LSTM\", \"BiLSTM\", \"GRU\", and \"BiGRU\" \n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the model name is one of the available models\n",
        "    assert model_name in [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\"]\n",
        "    ytest_df, ypred_df = turn_to_dataframe(model_name, dataset, ytest, ypred, percentage, step)\n",
        "    \n",
        "    plt.figure(figsize=(20, 15), dpi=500)\n",
        "    plt.grid(True)\n",
        "    plt.title(\"Prediction VS Actual for \" + model_name + \" model\")\n",
        "\n",
        "    plt.plot(ytest_df, color='red', label='Actual Price')\n",
        "    plt.plot(ypred_df, color='green', marker='.',label='Predicted Price')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Close Price ($)')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig('fig.png', format='png', dpi=500)\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wCdnDjjRCeK"
      },
      "source": [
        "# **Iterate and get the performance for all the models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5ph_CewxFJM"
      },
      "outputs": [],
      "source": [
        "# F1: Most correlated to the target variable\n",
        "F1 = ['open','high','low','adj_close','sma_indicator5','ema_indicator5','sma_indicator10',\n",
        "      'ema_indicator10','sma_indicator15','ema_indicator15','sma_indicator20','ema_indicator20',\n",
        "      'sma_indicator25','ema_indicator25','average_true_range25','sma_indicator30','ema_indicator30',\n",
        "      'average_true_range30','sma_indicator35','ema_indicator35','average_true_range35',\n",
        "      'sma_indicator40','ema_indicator40','average_true_range40','sma_indicator45','ema_indicator45',\n",
        "      'average_true_range45','sma_indicator50','ema_indicator50','average_true_range50','cumulative_return']\n",
        "\n",
        "# F2: F-regression test\n",
        "F2 = ['open', 'high', 'low', 'adj_close', 'sma_indicator5', 'ema_indicator5', 'sma_indicator10',\n",
        "      'ema_indicator10', 'sma_indicator15', 'ema_indicator15', 'sma_indicator20', 'ema_indicator20',\n",
        "      'ema_indicator25', 'ema_indicator30', 'cumulative_return']\n",
        "\n",
        "# F3: RandomForestRegressor\n",
        "F3 = ['open', 'high', 'low', 'adj_close', 'ema_indicator5', 'acc_dist_index', 'cumulative_return']\n",
        "\n",
        "# F4: Lasso's Regularization\n",
        "F4 = ['ema_indicator10', 'ema_indicator15', 'ema_indicator20', 'ema_indicator50', 'acc_dist_index']\n",
        "\n",
        "# F5: Bi-directional elimination(Step-wise Selection)\n",
        "F5  = ['cumulative_return', 'low', 'open', 'high']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gfSEQy8ve0Y"
      },
      "outputs": [],
      "source": [
        "# Initializing parameters\n",
        "models_name = [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\"]\n",
        "batch_size = [128, 256, 512, 1024]\n",
        "features_lists = [F1, F2, F3, F4, F5]\n",
        "use_dropout = [True, False]\n",
        "steps = [5, 10, 15, 20, 25, 30]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Test to get the best batch size and best set of features\n",
        "for name in models_name:\n",
        "  for drop in use_dropout:\n",
        "    for batch in batch_size:\n",
        "      for idx, feature in enumerate(features_lists):\n",
        "        model_name = name + '_batch' + str(batch) + '_F' + str(idx+1) + '_DropOut' + str(drop)\n",
        "        with mlflow.start_run(run_name = model_name):\n",
        "          ytest, ypred, model, model_history = model_DoItAll(name, df, feature, 5, 1000, batch, drop, 0.8)\n",
        "          print(\"\\nModel \" + model_name + \" Completed.\\n\")\n",
        "          mlflow.log_param(\"percentage\", 0.8)\n",
        "          mlflow.log_param(\"n_steps\", 5)\n",
        "          mlflow.log_param(\"epochs\", 1000)\n",
        "          mlflow.log_param(\"batch_size\", batch)\n",
        "          mlflow.log_param(\"use_dropout\", drop)\n",
        "          print('percentage: ', 0.8 ,'; n_steps: ', 5 ,'; epochs: ', 1000 ,'; batch_size: ', batch ,'; use_dropout: ', drop)\n",
        "\n",
        "          #mlflow.keras.log_model(model, model_name)\n",
        "          RMSE, MAPE, MAE = evaluate(ytest, ypred)\n",
        "          \n",
        "          mlflow.log_metric(\"RMSE\", RMSE)\n",
        "          mlflow.log_metric(\"MAPE\", MAPE)\n",
        "          mlflow.log_metric(\"MAE\", MAE)\n",
        "          print('RMSE: ', RMSE ,'; MAPE: ', MAPE ,'; MAE: ', MAE)\n",
        "\n",
        "          print(\"\\n Comparison Plot:\\n\")\n",
        "          make_plot(df, ytest, ypred, 0.8, name, 5)\n",
        "          np.save('ypred.npy', ypred)\n",
        "          mlflow.log_artifact('ypred.npy')\n",
        "          mlflow.log_artifact(\"fig.png\")\n",
        "\n",
        "          mlflow.end_run()"
      ],
      "metadata": {
        "id": "6DxGKfrXTbBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXambLVyvL7p"
      },
      "outputs": [],
      "source": [
        "## Test with all parameters\n",
        "for name in models_name:\n",
        "    for drop in use_dropout:\n",
        "      for batch in batch_size:\n",
        "        for step in steps:\n",
        "          for l in range(1, 11):\n",
        "            for idx, feature in enumerate(features_lists):\n",
        "              model_name = name + '_batch' + str(batch) + '_F5' + '_DropOut' + str(drop) + '_step' + str(step) + '_#' + str(l)\n",
        "              with mlflow.start_run(run_name = model_name):\n",
        "                ytest, ypred, model, model_history = model_DoItAll(name, df, feature, step, 1000, batch, drop, 0.8)\n",
        "\n",
        "                print(\"\\nModel \" + model_name + \" Completed.\\n\")\n",
        "                mlflow.log_param(\"percentage\", 0.8)\n",
        "                mlflow.log_param(\"n_steps\", step)\n",
        "                mlflow.log_param(\"epochs\", 1000)\n",
        "                mlflow.log_param(\"batch_size\", batch)\n",
        "                mlflow.log_param(\"use_dropout\", drop)\n",
        "                print('percentage: ', 0.8 ,'; n_steps: ', step ,'; epochs: ', 1000 ,'; batch_size: ', batch ,'; use_dropout: ', drop)\n",
        "\n",
        "                mlflow.keras.log_model(model, model_name)\n",
        "                RMSE, MAPE, MAE = evaluate(ytest, ypred)\n",
        "                \n",
        "                mlflow.log_metric(\"RMSE\", RMSE)\n",
        "                mlflow.log_metric(\"MAPE\", MAPE)\n",
        "                mlflow.log_metric(\"MAE\", MAE)\n",
        "                print('RMSE: ', RMSE ,'; MAPE: ', MAPE ,'; MAE: ', MAE)\n",
        "\n",
        "                print(\"\\n Comparison Plot:\\n\")\n",
        "                make_plot(df, ytest, ypred, 0.8, name, step)\n",
        "                np.save('ypred.npy', ypred)\n",
        "                mlflow.log_artifact('ypred.npy')\n",
        "                mlflow.log_artifact(\"fig.png\")\n",
        "\n",
        "                mlflow.end_run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HakE_Ywzzp28",
        "CN2bEwTL56Nj",
        "mkvPASvQQMGn",
        "aDDq5cUYPFkb",
        "2wCdnDjjRCeK"
      ],
      "name": "RNN_Forecasting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}